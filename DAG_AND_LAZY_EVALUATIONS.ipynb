{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d73f2717-4193-4c60-a56d-32c0923aa4eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+\n|DEST_COUNTRY_NAME|sum(count)|\n+-----------------+----------+\n|    United States|    384932|\n+-----------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "#Read Data - Job created for data reading action\n",
    "flight_df = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\",\"true\")\\\n",
    "            .option(\"inferschema\",\"true\")\\\n",
    "            .load(\"/FileStore/tables/data.csv\")  #Laod Data - Job created for data loading\n",
    "#Wide Dependency - Lazy Loading\n",
    "flight_df_repartition = flight_df.repartition(3)\n",
    "#Narrow Dependency - Lazy Loading\n",
    "us_flight_df = flight_df.filter(\"DEST_COUNTRY_NAME=='United States'\")\n",
    "#Narrow Dependency - Lazy Loading\n",
    "us_india_df = flight_df.filter((col(\"ORIGIN_COUNTRY_NAME\")=='India') | (col(\"ORIGIN_COUNTRY_NAME\")=='Singapore'))\n",
    "#Wide Dependency - Lazy Loading\n",
    "total_flight_ind_sing = us_flight_df.groupby(\"DEST_COUNTRY_NAME\").sum(\"count\")\n",
    "#Actions - Jobs Created and Executed\n",
    "total_flight_ind_sing.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4792af2b-9869-4d44-9fce-6d77c37488a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8]\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd_filtered = rdd.filter(lambda x : x%2 == 0)\n",
    "rdd_filtered = rdd_filtered.map(lambda x : x*2)\n",
    "result = rdd_filtered.collect()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DAG_AND_LAZY_EVALUATIONS",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
